{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ffc03e-c3a6-488c-b1ae-aa9cfbf1be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import time\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import csv\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import string\n",
    "import pandas as pd\n",
    "from urllib.parse import urljoin\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import schedule\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import itertools\n",
    "from spellchecker import SpellChecker\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878d2a2c-48bd-4960-b767-fb303be457f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sahil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Downloading necessary resources for pre-processing\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc26809-8072-4449-8c74-f27c59b6abd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "res=requests.get('https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/')\n",
    "print(res)\n",
    "soup=BeautifulSoup(res.text,'html.parser')\n",
    "#print(soup.prettify)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde030ff-262e-4d1d-a744-2e6f32857d38",
   "metadata": {},
   "source": [
    "<h1>##Checking robots file for allowed and disallowed websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d2f531b-2682-4707-b58f-1272ecb8ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the robots.txt file\n",
    "rp = RobotFileParser()\n",
    "root_url = 'https://pureportal.coventry.ac.uk'\n",
    "rp.set_url(urljoin(root_url, \"/robots.txt\"))\n",
    "rp.read()\n",
    "def is_allowed(url):\n",
    "    return rp.can_fetch(\"*\", url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2dfabc-4460-4125-9b68-77f84db2d5ee",
   "metadata": {},
   "source": [
    "<h1>##Crawling CSM Members page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6ef80b-70dd-46ae-b02b-6f77f01064af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_persons():\n",
    "    csm_member_links=set()\n",
    "    url='https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/persons/'\n",
    "    profile=requests.get(url)\n",
    "    if is_allowed(url):\n",
    "        profile_soup=BeautifulSoup(profile.text,'html.parser')\n",
    "        tiles=profile_soup.select('.result-container')\n",
    "        for profiles in tiles:\n",
    "            profile_links=profiles.find(\"a\", class_=\"link person\").get('href')\n",
    "            csm_member_links.add(profile_links)\n",
    "        return csm_member_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91be635c-a82e-4349-839c-12cf17b02e3c",
   "metadata": {},
   "source": [
    "<h1>#authors and their publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0602d3e4-d93b-42fc-ba93-6378294a2a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_author_pub_and_count(pub_author_url_count):\n",
    "    author_pub_and_count = dict()\n",
    "    for key in list(pub_author_url_count.keys()):\n",
    "        authors_portal_page = requests.get('https://pureportal.coventry.ac.uk/en/persons/'+key)\n",
    "        author_soup = BeautifulSoup(authors_portal_page.text, \"html.parser\")\n",
    "        author_name=author_soup.find('h1').text  \n",
    "        author_pub_and_count[author_name] = pub_author_url_count[key]\n",
    "            \n",
    "    #print(author_pub_and_count)    \n",
    "    with open('article_counts.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Author Name', 'No. of Articles Published'])\n",
    "        for author, count in author_pub_and_count.items():\n",
    "            writer.writerow([author, count])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0341f7-483a-44f4-bf40-868c80d5a811",
   "metadata": {},
   "source": [
    "<h1>##Crawling Websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ae0c363-0804-44a4-abc2-5a35e4af5824",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawler(root, page_count):\n",
    "    search_list=[]\n",
    "    visited_urls = set()\n",
    "    queue = [root] #this is the queue which initially contains the given seed URL\n",
    "    count = 0\n",
    "    csm_member_links=crawl_persons()\n",
    "    while(queue!=[] and count < page_count):\n",
    "        url = queue.pop(0)\n",
    "        if is_allowed(url):\n",
    "            if url in visited_urls:\n",
    "                continue\n",
    "            visited_urls.add(url)\n",
    "            print(\"fetching \" + url)\n",
    "            count +=1\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "            tiles=soup.select('.result-container')\n",
    "\n",
    "            c=1\n",
    "            for subsections in tiles:\n",
    "\n",
    "                list_of_records={}\n",
    "                members = subsections.find_all(\"a\", class_=\"link person\")\n",
    "                is_member_csm=False\n",
    "                for member in members:\n",
    "                    #print(member.get('href'))\n",
    "                    if member.get('href') in csm_member_links:\n",
    "                        #print('----------')\n",
    "                        #print(member.get('href'))\n",
    "                        is_member_csm=True\n",
    "                        break\n",
    "                #print(is_member_csm)\n",
    "                #count+=1\n",
    "                    #member_link=members.find('a',class_=\"link person\").get('href')\n",
    "                    #print(member_link)\n",
    "\n",
    "\n",
    "                if is_member_csm:\n",
    "                    p_title=subsections.find('a',{'class':'link'}).get_text()\n",
    "                    p_link=subsections.find('a',{'class':'link'}).get('href')\n",
    "                    p_date=subsections.find('span',{'class':'date'}).get_text()\n",
    "                    p_auth_names=[]\n",
    "                    p_auth_portals=[]\n",
    "\n",
    "\n",
    "                    for auth_info in subsections.findAll('a',{'class':'link person'}):\n",
    "                        p_auth_name=auth_info.get_text()\n",
    "                        p_auth_portal_link=auth_info.get('href')\n",
    "                        p_auth_names.append(p_auth_name)\n",
    "                        p_auth_portals.append(p_auth_portal_link)\n",
    "                        \n",
    "                        ##for author and their publishing count\n",
    "                        p_auth_portal_link=p_auth_portal_link.split('/')[-1]\n",
    "                        if p_auth_portal_link in pub_author_url_count:\n",
    "                            pub_author_url_count[p_auth_portal_link] += 1\n",
    "                        else:\n",
    "                            pub_author_url_count[p_auth_portal_link] = 1\n",
    "                        \n",
    "                    list_of_records['Name of Publication']=p_title\n",
    "                    list_of_records['Publication Link']=p_link\n",
    "                    list_of_records['Publication Date']=p_date\n",
    "                    list_of_records['List of Authors']=p_auth_names\n",
    "                    list_of_records['Author Pureportal Profile Link']=p_auth_portals\n",
    "                    search_list.append(list_of_records)\n",
    "                    c=c+1\n",
    "        \n",
    "        else:\n",
    "            print(f'{url} is not allowed to be crawled')\n",
    "\n",
    "        #print(author_pub_and_count)        \n",
    "        for nextpage in soup.findAll('a',attrs={\"class\": \"step\"}):\n",
    "            new_url = nextpage.get('href')\n",
    "            if(new_url != None and new_url != '/'):\n",
    "                new_url = urljoin(url, new_url)\n",
    "                #print(\"new_url is : \", new_url)  #uncomment the print statement to see the urls\n",
    "                queue.append(new_url)\n",
    "\n",
    "        # Sleep for a few seconds to avoid hitting the server too fast\n",
    "        time.sleep(rp.crawl_delay('*'))\n",
    "\n",
    "        headers=['Name of Publication','Publication Link','Publication Date','List of Authors','Author Pureportal Profile Link']\n",
    "        with open('output.csv', mode='w', newline='', encoding='utf-8') as output_file:\n",
    "            writer = csv.DictWriter(output_file, fieldnames=headers)\n",
    "            writer.writeheader()\n",
    "            for record in search_list:\n",
    "                writer.writerow(record)\n",
    "    #print(pub_author_url_count)\n",
    "    crawl_author_pub_and_count(pub_author_url_count)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0230af-881b-4b8b-b22d-0da97109e0a4",
   "metadata": {},
   "source": [
    "<h1> ##Schedule the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a8761b5-13e3-41e0-97aa-00a3a47fcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_crawler():\n",
    "    crawl_persons()\n",
    "    print(\"Crawler running at\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    crawler('https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/',10)\n",
    "    #if it is running on friday\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(60) # Wait for 1 minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf001883-9718-41d1-b39f-f4edf91e45c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select the following options\n",
      "1. To Run Crawler \n",
      "2. Schedule the crawler to run every Friday\n",
      "3. Exit\n",
      " 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/\n",
      "{'Miao Lin Pay': 1, 'Jesper Christensen': 1, 'Fei He': 7, 'Laura Roden': 2, 'Majdi Fanous': 2, 'Jonathan Eden': 2, 'Alireza Daneshkhah': 3, 'Maria Tariq': 1, 'Vasile Palade': 12, 'YingLiang Ma': 1, 'Xiaorui Jiang': 3, 'Sivasharmini Ganeshamoorthy': 1, 'Dominik Klepl': 4, 'Shenal Rajintha Alexander Samarathunge Gunawardena': 1, 'Huma Shah': 1, 'Pengpeng Hu': 1, 'AmirHosein Sadeghimanesh Sadeghi Manesh': 4, 'Jonathan Nixon': 1, 'Elena Gaura': 4, 'Alison Halford': 3, 'Michael Ajao-Olarinoye': 1, 'Ammar Al Bazi': 1, 'Matthew England': 3, 'Ankur Deo': 1, 'Matthew Stephen Tart': 1, 'Anup Pandey': 1, 'James Brusey': 2, 'Abiola Babatunde': 1, 'Brandi Jo Jess': 1, 'Beate Grawemeyer': 1, 'John Halloran': 1, 'Rashid Barket': 1, 'James Donnelly': 1, 'Omid Chatrabgoun': 1}\n",
      "Crawler running at 2023-03-30 02:51:24\n",
      "crawling completed\n",
      "\n",
      "2 files generated\n",
      "File path: C:\\Users\\Sahil\\Documents\\Information retrieval\\output.csv\n",
      "check output File path: C:\\Users\\Sahil\\Documents\\Information retrieval\\output.csv\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    x = input('Select the following options\\n1. To Run Crawler \\n2. Schedule the crawler to run every Friday\\n3. Exit\\n')\n",
    "    if x == '1':\n",
    "        pub_author_url_count=dict()\n",
    "        \n",
    "        crawl_persons()\n",
    "        crawler('https://pureportal.coventry.ac.uk/en/organisations/research-centre-for-computational-science-and-mathematical-modell/publications/', 1)\n",
    "\n",
    "        print(\"Crawler running at\", time.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        print('crawling completed')\n",
    "        #crawl_author_pub_and_count(auth_dict)\n",
    "        print()\n",
    "        print('2 files generated')\n",
    "        current_dir = os.path.abspath(os.curdir)\n",
    "        file_path = os.path.join(current_dir, 'output.csv')\n",
    "        print(\"File path:\", file_path)\n",
    "        current_dir = os.path.abspath(os.curdir)\n",
    "        file_path = os.path.join(current_dir, 'article_counts.csv')\n",
    "        print(\"check output File path:\", file_path)\n",
    "        break\n",
    "    elif x == '2':\n",
    "        schedule.every().friday.at('01:00').do(run_crawler)\n",
    "        print('crawler scheduled to run every friday at 01:00 AM')\n",
    "        print()\n",
    "    \n",
    "    elif x=='3':\n",
    "        break\n",
    "    else:\n",
    "        print('Choose the correct option')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99cf3eff-fbff-4bda-96ff-b6f5ff3ddea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path: C:\\Users\\Sahil\\Documents\\Information retrieval\\output.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f8bd991-f70a-4cc0-bc5e-48c420c8323b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of Publication</th>\n",
       "      <th>Publication Link</th>\n",
       "      <th>Publication Date</th>\n",
       "      <th>List of Authors</th>\n",
       "      <th>Author Pureportal Profile Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Extended Plant Circadian Clock Model for Ch...</td>\n",
       "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
       "      <td>9 Jan 2023</td>\n",
       "      <td>['Pay, M. L.', 'Christensen, J.', 'He, F.', 'R...</td>\n",
       "      <td>['https://pureportal.coventry.ac.uk/en/persons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Challenges and prospects of climate change imp...</td>\n",
       "      <td>https://pureportal.coventry.ac.uk/en/publicati...</td>\n",
       "      <td>25 Feb 2023</td>\n",
       "      <td>['Fanous, M.', 'Eden, J.', 'Daneshkhah, A.']</td>\n",
       "      <td>['https://pureportal.coventry.ac.uk/en/persons...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Name of Publication  \\\n",
       "0  An Extended Plant Circadian Clock Model for Ch...   \n",
       "1  Challenges and prospects of climate change imp...   \n",
       "\n",
       "                                    Publication Link Publication Date  \\\n",
       "0  https://pureportal.coventry.ac.uk/en/publicati...       9 Jan 2023   \n",
       "1  https://pureportal.coventry.ac.uk/en/publicati...      25 Feb 2023   \n",
       "\n",
       "                                     List of Authors  \\\n",
       "0  ['Pay, M. L.', 'Christensen, J.', 'He, F.', 'R...   \n",
       "1       ['Fanous, M.', 'Eden, J.', 'Daneshkhah, A.']   \n",
       "\n",
       "                      Author Pureportal Profile Link  \n",
       "0  ['https://pureportal.coventry.ac.uk/en/persons...  \n",
       "1  ['https://pureportal.coventry.ac.uk/en/persons...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=pd.read_csv(\"output.csv\") \n",
    "output.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "259720fa-d419-4f2f-b09a-430075089573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extend': [0], 'plant': [0, 30], 'circadian': [0], 'clock': [0], 'model': [0, 1, 6, 30, 31, 38, 71, 78, 80, 97]}\n"
     ]
    }
   ],
   "source": [
    "##preprocessing\n",
    "#This code performs the following preprocessing steps:\n",
    "\n",
    "#Removes Unicode characters from the text.\n",
    "#Removes mentions (words starting with '@').\n",
    "#Converts the text into lowercase.\n",
    "#Removes punctuation marks.\n",
    "#Removes stop words (common words like \"the\", \"a\", \"an\", etc.).\n",
    "#Joins the remaining words back into a string.\n",
    "\n",
    "\n",
    "\n",
    "processed_pub_names = []\n",
    "def preprocess_text(pub_name):\n",
    "    # Removing non-ASCII characters\n",
    "    pub_name = pub_name.encode('ascii', 'ignore').decode()\n",
    "    # Removing mentions (starting with '@')\n",
    "    pub_name = re.sub(r'@\\w+', '', pub_name)\n",
    "    # Converting to lowercase\n",
    "    pub_name = pub_name.lower()\n",
    "    # Removing punctuation marks\n",
    "    pub_name = pub_name.translate(str.maketrans('', '', string.punctuation))\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pub_name = ' '.join(word for word in pub_name.split() if word not in stop_words)\n",
    "    # Stemming words\n",
    "    stemmer = PorterStemmer()\n",
    "    pub_name = ' '.join(stemmer.stem(word) for word in pub_name.split())\n",
    "    return pub_name\n",
    "\n",
    "# apply the pre-processing function to each publication name and store the results in the processed_pub_names list\n",
    "for pub_name in output['Name of Publication']:\n",
    "    processed_pub_name = preprocess_text(pub_name)\n",
    "    processed_pub_names.append(processed_pub_name)\n",
    "    \n",
    "# create an empty dictionary to hold the inverted index\n",
    "inverted_index = {}\n",
    "\n",
    "# iterate through each publication name and tokenize it\n",
    "for i, pub_name in enumerate(processed_pub_names):\n",
    "    tokens = pub_name.split()\n",
    "    # iterate through each token and update the inverted index\n",
    "    for token in tokens:\n",
    "        if token in inverted_index:\n",
    "            inverted_index[token].append(i)\n",
    "        else:\n",
    "            inverted_index[token] = [i]\n",
    "\n",
    "# print top 5 entries in the inverted index\n",
    "top_five = dict(itertools.islice(inverted_index.items(), 5))\n",
    "\n",
    "print(top_five)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d85efb3-6b2e-4aa8-8d97-64bee96f99a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahil\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tfid = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None, binary=False)\n",
    "tfid_matrix = tfid.fit_transform(processed_pub_names)\n",
    "terms = tfid.get_feature_names()\n",
    "dense_matrix = tfid_matrix.toarray()\n",
    "dense_list = dense_matrix.tolist()\n",
    "\n",
    "# Create a dataframe with the tf-idf values\n",
    "tf_idf_df = pd.DataFrame(dense_list, columns=terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "da775327-77e8-48ef-876f-78fc6ad633b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Publication Name:  machine larninng\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machin', 'learn']\n"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "inputquery = input('Enter Publication Name: ')\n",
    "enteredquery=inputquery\n",
    "corrected_query=[]\n",
    "for word in inputquery.split(' '):\n",
    "    corrected_word = spell(word)\n",
    "    corrected_query.append(corrected_word)\n",
    "inputquery=' '.join(corrected_query)\n",
    "\n",
    "clean_inputquery = preprocess_text(inputquery).split()\n",
    "    \n",
    "print(clean_inputquery)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "571c9769-9693-4db3-9cda-54276d893712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered Query: machine larninng\n",
      "showing results for machin learn\n",
      "\n",
      "Publication Name: Machine Learning for Computer Algebra\n",
      "Publication Link: https://pureportal.coventry.ac.uk/en/publications/machine-learning-for-computer-algebra\n",
      "Publication Date: 2022\n",
      "Authors: ['Barket, R.', 'del Río, T.', 'England, M.']\n",
      "Author Portal Links: ['https://pureportal.coventry.ac.uk/en/persons/rashid-barket', 'https://pureportal.coventry.ac.uk/en/persons/tereso-del-r%C3%ADo-almajano', 'https://pureportal.coventry.ac.uk/en/persons/matthew-england']\n",
      "***********************************************\n",
      "Publication Name: SC-Square: Future Progress with Machine Learning\n",
      "Publication Link: https://pureportal.coventry.ac.uk/en/publications/sc-square-future-progress-with-machine-learning\n",
      "Publication Date: 14 Nov 2022\n",
      "Authors: ['England, M.']\n",
      "Author Portal Links: ['https://pureportal.coventry.ac.uk/en/persons/matthew-england']\n",
      "***********************************************\n",
      "Publication Name: Using Machine Learning in SC2\n",
      "Publication Link: https://pureportal.coventry.ac.uk/en/publications/using-machine-learning-in-sc2\n",
      "Publication Date: 23 Aug 2022\n",
      "Authors: ['del Río, T.']\n",
      "Author Portal Links: ['https://pureportal.coventry.ac.uk/en/persons/tereso-del-r%C3%ADo-almajano']\n",
      "***********************************************\n",
      "Publication Name: A machine learning based software pipeline to pick the variable ordering for algorithms with polynomial inputs\n",
      "Publication Link: https://pureportal.coventry.ac.uk/en/publications/a-machine-learning-based-software-pipeline-to-pick-the-variable-o\n",
      "Publication Date: 8 Jul 2020\n",
      "Authors: ['England, M.']\n",
      "Author Portal Links: ['https://pureportal.coventry.ac.uk/en/persons/matthew-england']\n",
      "***********************************************\n",
      "Publication Name: A Leap from Randomized to Quantum Clustering with Support Vector Machine - A Computation Complexity Analysis\n",
      "Publication Link: https://pureportal.coventry.ac.uk/en/publications/a-leap-from-randomized-to-quantum-clustering-with-support-vector-\n",
      "Publication Date: 3 Aug 2020\n",
      "Authors: ['Palade, V.']\n",
      "Author Portal Links: ['https://pureportal.coventry.ac.uk/en/persons/vasile-palade']\n",
      "***********************************************\n",
      "Publication Name: Diabetic Retinopathy Detection Using Transfer and Reinforcement Learning with Effective Image Preprocessing and Data Augmentation Techniques\n",
      "Publication Link: https://pureportal.coventry.ac.uk/en/publications/diabetic-retinopathy-detection-using-transfer-and-reinforcement-l\n",
      "Publication Date: 7 Feb 2023\n",
      "Authors: ['Tariq, M.', 'Palade, V.', 'Ma, Y.']\n",
      "Author Portal Links: ['https://pureportal.coventry.ac.uk/en/persons/maria-tariq', 'https://pureportal.coventry.ac.uk/en/persons/vasile-palade', 'https://pureportal.coventry.ac.uk/en/persons/yingliang-ma']\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "# Calculate match scores\n",
    "match_scores = [sum(tf_idf_df.iloc[i][j] for j in clean_inputquery if j in tf_idf_df.columns) for i in range(len(processed_pub_names))]\n",
    "\n",
    "# Check if there are any matches\n",
    "if all(score == 0 for score in match_scores):\n",
    "    print(\"No related research paper found.\")\n",
    "else:\n",
    "    # Sort results by match score and select top 6\n",
    "    top_results = sorted(enumerate(match_scores), key=lambda x: x[1], reverse=True)[:6]\n",
    "    \n",
    "    # Print results\n",
    "    print('Entered Query:',enteredquery)\n",
    "    print('showing results for',' '.join(clean_inputquery))\n",
    "    print()\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    for i, score in top_results:\n",
    "        row = output.iloc[i]\n",
    "        publication_name = row['Name of Publication'].strip()\n",
    "        publication_link = row['Publication Link'].strip()\n",
    "        publication_date = row['Publication Date'].strip()\n",
    "        authors = row['List of Authors'].strip()\n",
    "        authors_portal_links = row['Author Pureportal Profile Link'].strip()\n",
    "        print('Publication Name:', publication_name)\n",
    "        print('Publication Link:', publication_link)\n",
    "        print('Publication Date:', publication_date)\n",
    "        print('Authors:', authors)\n",
    "        print('Author Portal Links:', authors_portal_links)\n",
    "        print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9544b7a-6e24-4bee-bf17-27ab1b5f5ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
